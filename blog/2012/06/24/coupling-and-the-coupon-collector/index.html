<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Coupling and the Coupon Collector | kennary island</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Coupling and the Coupon Collector" />
<meta name="author" content="Kenny Yu" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="In the fall semester last year, I took Stat 110, an introductory statistics course focusing on probability. I had done probability in contest math from high school, but this course was my first real rigorous treatment of probability." />
<meta property="og:description" content="In the fall semester last year, I took Stat 110, an introductory statistics course focusing on probability. I had done probability in contest math from high school, but this course was my first real rigorous treatment of probability." />
<link rel="canonical" href="http://kennyyu.me/blog/2012/06/24/coupling-and-the-coupon-collector/" />
<meta property="og:url" content="http://kennyyu.me/blog/2012/06/24/coupling-and-the-coupon-collector/" />
<meta property="og:site_name" content="kennary island" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2012-06-24T21:43:00-07:00" />
<script type="application/ld+json">
{"description":"In the fall semester last year, I took Stat 110, an introductory statistics course focusing on probability. I had done probability in contest math from high school, but this course was my first real rigorous treatment of probability.","@type":"BlogPosting","headline":"Coupling and the Coupon Collector","dateModified":"2012-06-24T21:43:00-07:00","datePublished":"2012-06-24T21:43:00-07:00","url":"http://kennyyu.me/blog/2012/06/24/coupling-and-the-coupon-collector/","mainEntityOfPage":{"@type":"WebPage","@id":"http://kennyyu.me/blog/2012/06/24/coupling-and-the-coupon-collector/"},"author":{"@type":"Person","name":"Kenny Yu"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://kennyyu.me/feed.xml" title="kennary island" /><script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        processEscapes: true
      }
    });
  </script>
</head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">kennary island</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/archives/">Archives</a><a class="page-link" href="/projects/">Projects</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Coupling and the Coupon Collector</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2012-06-24T21:43:00-07:00" itemprop="datePublished">Jun 24, 2012
      </time>• <span itemprop="author" itemscope itemtype="http://schema.org/Person"><span class="p-author h-card" itemprop="name">Kenny Yu</span></span></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p>In the fall semester last year, I took <a href="http://blog.bokcenter.harvard.edu/2012/01/23/stat-110-lectures-are-now-available-on-itunesu/">Stat 110</a>,
an introductory
statistics course focusing on probability. I had done probability in contest
math from high school, but this course was my first real rigorous treatment
of probability.</p>

<p>One of the most interesting problems I saw in the course involves Markov chains
and a simple and elegant solution using another interesting problem we saw
earlier in the course–<a href="http://en.wikipedia.org/wiki/Coupon_collector%27s_problem">the coupon collector’s problem</a>.</p>

<h2 id="random-to-top-shuffling-problem">Random-to-Top Shuffling Problem</h2>

<p>Suppose $n$ cards are placed in order on a table. Consider the following shuffling
procedure: Pick a card at random from the deck and place it on top of the deck.
What is the expected number of times we need to repeat the process to arrive
at a “random” deck, for some suitable definition of “random”?</p>

<p>To solve this question, we’ll need to answer a seemingly unrelated question first.</p>

<h2 id="coupon-collectors-problem-aka-the-toy-collectors-problem">Coupon Collector’s Problem (aka. The Toy Collector’s Problem)</h2>

<p>A certain brand of cereal always distributes a toy in every cereal box. The toy
chosen for each box is chosen randomly from a set of $n$ distinct toys. A toy
collector wishes to collect all $n$ distinct toys. What is the expected number
of cereal boxes must the toy collector buy so that the toy collector collects
all $n$ distinct toys?</p>

<h2 id="solution-to-the-toy-collectors-problem">Solution to the Toy Collector’s Problem</h2>

<p>The key to understanding this problem is to break the task of collecting all
$n$ distinct toys into different stages: what is the expected number of cereal
boxes that the toy collector has to buy to get the $i$-th new toy?</p>

<p>Let random variable $X_i$ be the number of boxes it takes for the toy collector
to collect the $i$-th new toy after the $i-1$-th toy has already been collected.
(Note: this does NOT mean assign numbers to toys
and then collect the $i$-th toy. Instead, this means that after $X_i$ boxes,
the toy collector would have collected $i$ distinct toys, but with only $X_i - 1$
boxes, the toy collector would have only collected $i-1$ distinct toys.)</p>

<p>Clearly $E(X_1) = 1$, because the toy collector starts off with no toys. Now
consider the $i$-th toy. After the $i-1$-th toy has been collected, then there
are $n - (i-1)$ possible toys that could be the new $i$-th toy. We can interpret
the process of waiting for the $i$-th new toy as a <a href="http://en.wikipedia.org/wiki/Geometric_distribution">geometric distribution</a>, where
each trial is buying another cereal box, “success” is getting any of the
$n - (i-1)$ uncollected toys, and
“failure” is getting any of the already collected $i - 1$ toys. From this point
of view, we see that</p>

<script type="math/tex; mode=display">X_i - 1 \sim \textrm{Geom}(p)</script>

<p>where the probability of success $p$ is</p>

<script type="math/tex; mode=display">p = \frac{n - (i-1)}{n}.</script>

<p>Here, our definition of the geometric distribution does NOT include the success.
Using the expectation of a geometric distribution, we have that the expected number
of cereal boxes the toy collector must collect to get the $i$-th new toy after
collecting $i-1$ toys is</p>

<script type="math/tex; mode=display">E(X_i - 1) = \frac{1 - p}{p}</script>

<script type="math/tex; mode=display">E(X_i) = \frac{1}{p} = \frac{n}{n - (i - 1)}.</script>

<p>Now let random variable $X$ to be the number of cereal boxes the toy collector
needs to buy to collect all $n$ distinct toys. Since we have separated the
process into collecting the $i$-th new toy, then</p>

<script type="math/tex; mode=display">X = X_1 + X_2 + \cdots + X_n.</script>

<p>Using linearity of expectation, we can compute the expected value of $X$
by summing the individual expectations of $X_i$. Thus, we obtain the following
result:</p>

<script type="math/tex; mode=display">E(X) = E(X_1 + X_2 + X_3 + \cdots + X_n)</script>

<script type="math/tex; mode=display">E(X) = E(X_1) + E(X_2) + E(X_3) + \cdots + E(X_n)</script>

<script type="math/tex; mode=display">E(X) = n\left( 1 + \frac{1}{2} + \frac{1}{3} + \cdots + \frac{1}{n} \right).</script>

<p>This is the harmonic series! The harmonic series diverges to infinity and
grows approximately as $\gamma + \log n$ where $\gamma \approx 0.57722$ is
Euler’s constant. Thus, we can approximate the expected number of cereal boxes with:</p>

<script type="math/tex; mode=display">E(X) \approx n (\gamma + \log n).</script>

<h2 id="solution-to-the-random-to-top-shuffling-problem">Solution to the Random-to-Top Shuffling Problem</h2>

<h3 id="markov-chains-and-stationary-distrubutions">Markov Chains and Stationary Distrubutions</h3>

<p>Coming back to the random-to-top shuffling problem, we first need to define
our notion of “random” for our deck. In order to do this, we use Markov chains.</p>

<p>For our Markov chain, let our states be all $n!$ permutations of $n$-card deck,
and two states are adjacent if and only if it is possible to reach one of the
states from the other through one step of this shuffle. For any state, we move to
one of its $n-1$ neighbors with probability $\frac{1}{n}$, or stay at the same
state with probability $\frac{1}{n}$. Since all of our $n!$ states has degree
$n$ (including loops), then by symmetry, the probability of having any
permutation is equally likely. Thus, the stationary distribution for our
random-to-top shuffling Markov process is the uniform vector</p>

<script type="math/tex; mode=display">\vec{s} = \left(\frac{1}{n!}, ..., \frac{1}{n!}\right).</script>

<p>Thus, to define our notion of a “random” deck, we would like that after
implementing our shuffling algorithm, the resulting deck is sampled from our
stationary distribution: that is, our resulting deck is equally likely
to be any of the $n!$ permutations.</p>

<h3 id="coupling">Coupling</h3>

<p>Now that we have established that our shuffling process can be modeled with
a Markov chain that has a stationary distribution, we use the idea of “coupling”
to arrive at our solution.</p>

<p>Let deck $A$ be our original deck, and let deck $B$ be uniformly randomly sampled
from all $n!$ permutations. Since the stationary distribution for our shuffling
process is the uniform distribution, then deck $B$ is sampled from the
stationary distribution.</p>

<p>We use the fact that if we start our Markov process from a state sampled
from the stationary distribution, then the resulting state will also be
sampled from the stationary distribution. More formally:</p>

<p><strong>Lemma.</strong> <em>Let $\vec{s}$
be the stationary distribution of our Markov chain.
Let $X_0$ be our starting state, and let it
be sampled from the stationary distribution (i.e. $P(X_0 = i) = s_i$). Then
the resulting state $X_1$ after running the Markov chain for one step
will also be sampled from $\vec{s}$</em>.</p>

<p>Now consider our “coupling” strategy: every time we move a card $C$ to the top
of deck $A$, we locate card $C$ in deck $B$ and place it on top of the deck.
Note that the physical process of how we chose card $C$ in the two decks is
different: we choose a random position in deck $A$, whereas we located card
$C$ in deck $B$. Although the process of how we chose card $C$ is different,
from deck $B$’s perspective, $C$ is simply a card selected at random. Using
our lemma, we have that deck $B$ still remains sampled from the stationary
distribution after moving card $C$ to the top of deck $B$.</p>

<p>We note that after $t$ steps, all the cards that have been touched
up to time $t$ will be in the same order on top of both decks.
When all the cards of deck A and deck B are in the same order
after some time $T$ steps, we will have that deck A and deck B are both
sampled from the stationary distribution (because B always stays
stationary through our coupling strategy). Thus, after $T$ steps,
deck A will satisfy our notion of a “random” deck. We wish to compute $E(T)$.</p>

<p>How do we compute $E(T)$? We note that both decks will be the same once we have
touched all the cards. Therefore, we wish to compute the expected number
of random-to-top shuffles needed to touch all the cards.
This is an instance of the coupon collector’s problem! Instead of
touching all $n$ cards, we wish the collect all $n$ coupons. Thus,
after approximately $n (\gamma + \log n)$
random-to-top shuffles, our original deck $A$
will be a “random” deck. For $n = 52$, we require $E(T) \approx 236$ shuffles
to randomize our deck.</p>

  </div><a class="u-url" href="/blog/2012/06/24/coupling-and-the-coupon-collector/" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">kennary island</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">kennary island</li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/kennyyu"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">kennyyu</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p></p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
